{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MyiBKHvm6Ss",
        "outputId": "24e11029-e153-417e-aed0-5f3029e31856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_vN065nnFsU"
      },
      "outputs": [],
      "source": [
        "path_to_data = \"/content/drive/My Drive/eICU_Data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hql1nGWonNQv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGhnnt0gK_-7",
        "outputId": "2bf74e7f-14f6-49c7-dcf7-611c2c00dfce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUHkgdGanQHH"
      },
      "outputs": [],
      "source": [
        "input_path = path_to_data + '/fold_0'\n",
        "model_dir = path_to_data + '/fold_0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-x0_T1encMR"
      },
      "outputs": [],
      "source": [
        "class SequenceExampleParser(object):\n",
        "    def __init__(self, batch_size, num_map_threads=4):\n",
        "        self.context_features_config = {\n",
        "            'patientId': tf.io.VarLenFeature(tf.string),\n",
        "            'label.readmission': tf.io.FixedLenFeature([1], tf.int64),\n",
        "            'label.expired': tf.io.FixedLenFeature([1], tf.int64)\n",
        "        }\n",
        "        self.sequence_features_config = {\n",
        "        'dx_ints': tf.io.VarLenFeature(tf.int64),\n",
        "        'proc_ints': tf.io.VarLenFeature(tf.int64),\n",
        "        'prior_indices': tf.io.VarLenFeature(tf.int64),\n",
        "        'prior_values': tf.io.VarLenFeature(tf.float32)\n",
        "        }\n",
        "        self.batch_size = batch_size\n",
        "        self.num_map_threads = num_map_threads\n",
        "    def __call__(self, tfrecord_path, label_key, training):\n",
        "        def parser_fn(serialized_example):\n",
        "            (batch_context, batch_sequence) = tf.io.parse_single_sequence_example(\n",
        "                serialized_example,\n",
        "                context_features=self.context_features_config,\n",
        "                sequence_features=self.sequence_features_config)\n",
        "            labels = tf.squeeze(tf.cast(batch_context[label_key], tf.float32))\n",
        "            return batch_context,batch_sequence, labels\n",
        "        num_epochs = 1\n",
        "        # num_epochs=1\n",
        "        # buffer_size = self.batch_size * 32\n",
        "        dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
        "\n",
        "        # dataset = dataset.shuffle(buffer_size)\n",
        "        dataset = dataset.repeat(num_epochs)\n",
        "        dataset = dataset.map(parser_fn, num_parallel_calls=self.num_map_threads)\n",
        "        dataset = dataset.batch(self.batch_size)\n",
        "        # dataset = dataset.prefetch(1)\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UQ6ZygXng2n"
      },
      "outputs": [],
      "source": [
        "seq_samp= SequenceExampleParser(batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9K6Q-EInskU"
      },
      "outputs": [],
      "source": [
        "label_key = 'label.readmission'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsAvdHp_nwqI"
      },
      "outputs": [],
      "source": [
        "train_dataset = seq_samp(input_path + '/train.tfrecord',label_key, True)\n",
        "test_dataset = seq_samp(input_path + '/train.tfrecord',label_key, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKa0IHjk3ser"
      },
      "outputs": [],
      "source": [
        "# adv_samples=test_dataset.take(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbUhyLFH307D"
      },
      "outputs": [],
      "source": [
        "# for c,f,l in adv_samples:\n",
        "#     print(c)\n",
        "#     print(f)\n",
        "#     print(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU6f2RfvYvWw",
        "outputId": "7ab2f3a6-0393-4b24-8678-c691fe9d37ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1026\n"
          ]
        }
      ],
      "source": [
        "total_samples = 0\n",
        "for c,f,l in train_dataset:\n",
        "    total_samples += 1\n",
        "print(total_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H0L9ex99Oe5"
      },
      "outputs": [],
      "source": [
        "class GraphConvolutionalTransformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "               embedding_size=128,\n",
        "               num_transformer_stack=3,\n",
        "               num_feedforward=2,\n",
        "               num_attention_heads=1,\n",
        "               ffn_dropout=0.08,\n",
        "               attention_normalizer='softmax',\n",
        "               multihead_attention_aggregation='concat',\n",
        "               directed_attention=False,\n",
        "               use_inf_mask=True,\n",
        "               use_prior=True,\n",
        "               num_classes=1,\n",
        "               **kwargs):\n",
        "        super(GraphConvolutionalTransformer, self).__init__(**kwargs)\n",
        "        self._hidden_size = embedding_size\n",
        "        self._num_stack = num_transformer_stack\n",
        "        self._num_classes=num_classes\n",
        "        self._num_feedforward = num_feedforward\n",
        "        self._num_heads = num_attention_heads\n",
        "        self._ffn_dropout = ffn_dropout\n",
        "        self._attention_normalizer = attention_normalizer\n",
        "        self._multihead_aggregation = multihead_attention_aggregation\n",
        "        self._directed_attention = directed_attention\n",
        "        self._use_inf_mask = use_inf_mask\n",
        "        self._use_prior = use_prior\n",
        "        self.layer_norms = [tf.keras.layers.LayerNormalization(axis=2) for _ in range(num_transformer_stack)]\n",
        "\n",
        "        self._layers = {}\n",
        "        self._layers['Q'] = []\n",
        "        self._layers['K'] = []\n",
        "        self._layers['V'] = []\n",
        "        self._layers['ffn'] = []\n",
        "        self._layers['head_agg'] = []\n",
        "        self._layers['dense']=tf.keras.layers.Dense(self._num_classes, activation=None)\n",
        "        for i in range(self._num_stack):\n",
        "            self._layers['Q'].append(\n",
        "                tf.keras.layers.Dense(\n",
        "                    self._hidden_size * self._num_heads, use_bias=False,name=f'query_dense_{i}'))\n",
        "            self._layers['K'].append(\n",
        "                tf.keras.layers.Dense(\n",
        "                    self._hidden_size * self._num_heads, use_bias=False,name=f'K_dense_{i}'))\n",
        "            self._layers['V'].append(\n",
        "                tf.keras.layers.Dense(\n",
        "                    self._hidden_size * self._num_heads, use_bias=False,name=f'V_dense_{i}'))\n",
        "            if self._multihead_aggregation == 'concat':\n",
        "                self._layers['head_agg'].append(\n",
        "                tf.keras.layers.Dense(self._hidden_size, use_bias=False,name=f'head_agg_{i}'))\n",
        "            self._layers['ffn'].append([])\n",
        "            for _ in range(self._num_feedforward - 1):\n",
        "                self._layers['ffn'][i].append(\n",
        "                    tf.keras.layers.Dense(self._hidden_size, activation='relu',name=f'ffn_dense_{i}'))\n",
        "            self._layers['ffn'][i].append(tf.keras.layers.Dense(self._hidden_size))\n",
        "\n",
        "    def feedforward(self, features,stack_index,training=None):\n",
        "        for i in range(self._num_feedforward):\n",
        "            features=self._layers['ffn'][stack_index][i](features)\n",
        "            if training:\n",
        "                features = tf.nn.dropout(features, rate=self._ffn_dropout)\n",
        "        return features\n",
        "\n",
        "    def qk_op(self,\n",
        "            features,\n",
        "            stack_index,\n",
        "            batch_size,\n",
        "            num_codes,\n",
        "            attention_mask,\n",
        "            inf_mask=None,\n",
        "            directed_mask=None):\n",
        "        q=self._layers['Q'][stack_index](features)\n",
        "        q = tf.reshape(q,\n",
        "                   [batch_size, num_codes, self._hidden_size, self._num_heads])\n",
        "        k = self._layers['K'][stack_index](features)\n",
        "        k = tf.reshape(k,\n",
        "                   [batch_size, num_codes, self._hidden_size, self._num_heads])\n",
        "        q = tf.transpose(q, perm=[0, 3, 1, 2])\n",
        "        k = tf.transpose(k, perm=[0, 3, 2, 1])\n",
        "        pre_softmax = tf.matmul(q, k) / tf.sqrt(\n",
        "            tf.cast(self._hidden_size, tf.float32))\n",
        "        pre_softmax -= attention_mask[:, None, None, :]\n",
        "        if inf_mask is not None:\n",
        "            pre_softmax -= inf_mask[:, None, :, :]\n",
        "\n",
        "        if directed_mask is not None:\n",
        "            pre_softmax -= directed_mask\n",
        "\n",
        "        if self._attention_normalizer == 'softmax':\n",
        "            attention = tf.nn.softmax(pre_softmax, axis=3)\n",
        "        else:\n",
        "            attention = tf.nn.sigmoid(pre_softmax)\n",
        "        return attention\n",
        "\n",
        "    @tf.function\n",
        "    def get_embeddings(self,features,max_num_codes):\n",
        "        embedding_dict, mask_dict = feature_embedder.lookup(features, max_num_codes)\n",
        "        keys = ['visit'] + feature_keys\n",
        "        embeddings = tf.concat([embedding_dict[key] for key in keys], axis=1)\n",
        "        masks = tf.concat([mask_dict[key] for key in keys], axis=1)\n",
        "        guide, prior_guide = create_matrix_vdp(features, masks, gct_params[\"use_prior\"],\n",
        "                                                gct_params[\"use_inf_mask\"],\n",
        "                                                max_num_codes,\n",
        "                                                prior_scalar)\n",
        "        masks=masks[:, :, None]\n",
        "        return embeddings,masks,guide,prior_guide\n",
        "    @tf.function\n",
        "    def call(self, features,masks,guide,prior_guide,training=None):\n",
        "        batch_size = tf.shape(features)[0]\n",
        "        num_codes = tf.shape(features)[1]\n",
        "        mask_idx = tf.cast(tf.where(tf.equal(masks[:, :, 0], 0.)), tf.int32)\n",
        "        mask_matrix = tf.fill([tf.shape(mask_idx)[0]], tf.float32.max)\n",
        "        attention_mask = tf.scatter_nd(\n",
        "        indices=mask_idx, updates=mask_matrix, shape=tf.shape(masks[:, :, 0]))\n",
        "        inf_mask = None\n",
        "        if self._use_inf_mask:\n",
        "            guide_idx = tf.cast(tf.where(tf.equal(guide, 0.)), tf.int32)\n",
        "            inf_matrix = tf.fill([tf.shape(guide_idx)[0]], tf.float32.max)\n",
        "            inf_mask = tf.scatter_nd(\n",
        "                indices=guide_idx, updates=inf_matrix, shape=tf.shape(guide))\n",
        "        directed_mask = None\n",
        "        if self._directed_attention:\n",
        "            inf_matrix = tf.fill([num_codes, num_codes], tf.float32.max)\n",
        "            inf_matrix = tf.matrix_set_diag(inf_matrix, tf.zeros(num_codes))\n",
        "            directed_mask = tf.matrix_band_part(inf_matrix, -1, 0)[None, None, :, :]\n",
        "        attention = None\n",
        "        attentions = []\n",
        "        for i, layer_norm in enumerate(self.layer_norms):\n",
        "            features = masks * features\n",
        "\n",
        "            if self._use_prior and i == 0:\n",
        "                attention = tf.tile(prior_guide[:, None, :, :],\n",
        "                                    [1, self._num_heads, 1, 1])\n",
        "            else:\n",
        "                attention = self.qk_op(features, i, batch_size, num_codes,\n",
        "                                    attention_mask, inf_mask, directed_mask)\n",
        "            attentions.append(attention)\n",
        "            v = self._layers['V'][i](features)\n",
        "            v = tf.reshape(\n",
        "                v, [batch_size, num_codes, self._hidden_size, self._num_heads])\n",
        "            v = tf.transpose(v, perm=[0, 3, 1, 2])\n",
        "            # post_attention is (batch, num_heads, num_codes, hidden_size)\n",
        "            post_attention = tf.matmul(attention, v)\n",
        "            if self._num_heads == 1:\n",
        "                post_attention = tf.squeeze(post_attention, axis=1)\n",
        "            elif self._multihead_aggregation == 'concat':\n",
        "                # post_attention is (batch, num_codes, num_heads, hidden_size)\n",
        "                post_attention = tf.transpose(post_attention, perm=[0, 2, 1, 3])\n",
        "                # post_attention is (batch, num_codes, num_heads*hidden_size)\n",
        "                post_attention = tf.reshape(post_attention, [batch_size, num_codes, -1])\n",
        "                # post attention is (batch, num_codes, hidden_size)\n",
        "                post_attention = self._layers['head_agg'][i](post_attention)\n",
        "            else:\n",
        "                post_attention = tf.reduce_sum(post_attention, axis=1)\n",
        "            post_attention += features\n",
        "            post_attention = layer_norm(post_attention)\n",
        "            post_ffn = self.feedforward(post_attention, i, training)\n",
        "            post_ffn += post_attention\n",
        "            post_ffn = layer_norm(post_ffn)\n",
        "            features = post_ffn\n",
        "        hidden=features * masks\n",
        "        pre_logits=hidden[:,0,:]\n",
        "        pre_logits=tf.reshape(pre_logits,[-1,self._hidden_size])\n",
        "        logits=self._layers['dense'](pre_logits)\n",
        "        logits=tf.squeeze(logits)\n",
        "        return logits, attentions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRlJs4cOKvno"
      },
      "outputs": [],
      "source": [
        "class FeatureEmbedder(object):\n",
        "    def __init__(self, vocab_sizes, feature_keys, embedding_size):\n",
        "        self._params = {}\n",
        "        self._feature_keys = feature_keys\n",
        "        self._vocab_sizes = vocab_sizes\n",
        "        dummy_emb = tf.zeros([1, embedding_size], dtype=tf.float32)\n",
        "        for feature_key in feature_keys:\n",
        "            vocab_size = self._vocab_sizes[feature_key]\n",
        "            emb = tf.Variable(\n",
        "                initial_value=tf.random.normal([vocab_size, embedding_size]), dtype=tf.float32, name=feature_key)\n",
        "            self._params[feature_key] = tf.concat([emb, dummy_emb], axis=0)\n",
        "        self._params['visit'] = tf.Variable(\n",
        "            initial_value=tf.random.normal([1, embedding_size]), dtype=tf.float32, name='visit')\n",
        "    @tf.function\n",
        "    def lookup(self, feature_map, max_num_codes):\n",
        "        masks = {}\n",
        "        embeddings = {}\n",
        "        for key in self._feature_keys:\n",
        "            if max_num_codes > 0:\n",
        "                feature = tf.SparseTensor(\n",
        "                    indices=feature_map[key].indices,\n",
        "                    values=feature_map[key].values,\n",
        "                    dense_shape=[\n",
        "                        feature_map[key].dense_shape[0],\n",
        "                        feature_map[key].dense_shape[1], max_num_codes\n",
        "                    ])\n",
        "            else:\n",
        "                feature = feature_map[key]\n",
        "            feature_ids = tf.sparse.to_dense(\n",
        "                feature, default_value=self._vocab_sizes[key])\n",
        "            feature_ids = tf.squeeze(feature_ids, axis=1)\n",
        "            embeddings[key] = tf.nn.embedding_lookup(self._params[key], feature_ids)\n",
        "\n",
        "            mask = tf.SparseTensor(\n",
        "                indices=feature.indices,\n",
        "                values=tf.ones(tf.shape(feature.values)),\n",
        "                dense_shape=feature.dense_shape)\n",
        "            masks[key] = tf.squeeze(tf.sparse.to_dense(mask), axis=1)\n",
        "        batch_size = tf.shape(list(embeddings.values())[0])[0]\n",
        "        embeddings['visit'] = tf.tile(self._params['visit'][None, :, :],\n",
        "                                    [batch_size, 1, 1])\n",
        "\n",
        "        masks['visit'] = tf.ones(batch_size)[:, None]\n",
        "        # print(\"embeddings:\")\n",
        "        # print(embeddings)\n",
        "\n",
        "        return embeddings, masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2xkP6baLrwc"
      },
      "outputs": [],
      "source": [
        "gct_params = {\n",
        "    \"embedding_size\": 128,\n",
        "    \"num_transformer_stack\": 3,\n",
        "    \"num_feedforward\": 2,\n",
        "    \"num_attention_heads\": 1,\n",
        "    \"ffn_dropout\": 0.08,\n",
        "    \"attention_normalizer\": \"softmax\",\n",
        "    \"multihead_attention_aggregation\": \"concat\",\n",
        "    \"directed_attention\": False,\n",
        "    \"use_inf_mask\": True,\n",
        "    \"use_prior\": True,\n",
        "}\n",
        "vocab_sizes={'dx_ints':3249, 'proc_ints':2210}\n",
        "feature_keys=['dx_ints', 'proc_ints']\n",
        "embedding_size = gct_params['embedding_size']\n",
        "reg_coef=0.1\n",
        "prior_scalar=0.5\n",
        "learning_rate=0.022\n",
        "max_num_codes=50\n",
        "batch_size=32\n",
        "feature_keys=['dx_ints', 'proc_ints']\n",
        "\n",
        "model=GraphConvolutionalTransformer(**gct_params)\n",
        "feature_embedder = FeatureEmbedder(\n",
        "                vocab_sizes, feature_keys, embedding_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbki0oZGLKch"
      },
      "outputs": [],
      "source": [
        "def create_matrix_vdp(features, mask, use_prior, use_inf_mask, max_num_codes,\n",
        "                    prior_scalar):\n",
        "    dx_ids = features['dx_ints']\n",
        "    proc_ids = features['proc_ints']\n",
        "\n",
        "    batch_size = dx_ids.dense_shape[0]\n",
        "    num_dx_ids = max_num_codes if use_prior else dx_ids.dense_shape[-1]\n",
        "    num_proc_ids = max_num_codes if use_prior else proc_ids.dense_shape[-1]\n",
        "    num_codes = 1 + num_dx_ids + num_proc_ids\n",
        "\n",
        "    guide = None\n",
        "    if use_inf_mask:\n",
        "        row0 = tf.concat([\n",
        "            tf.zeros([1, 1]),\n",
        "            tf.ones([1, num_dx_ids]),\n",
        "            tf.zeros([1, num_proc_ids])\n",
        "        ],axis=1)\n",
        "\n",
        "        row1 = tf.concat([\n",
        "            tf.zeros([num_dx_ids, 1 + num_dx_ids]),\n",
        "            tf.ones([num_dx_ids, num_proc_ids])\n",
        "        ],axis=1)\n",
        "\n",
        "        row2 = tf.zeros([num_proc_ids, num_codes])\n",
        "\n",
        "        guide = tf.concat([row0, row1, row2], axis=0)\n",
        "        guide = guide + tf.transpose(guide)\n",
        "        guide = tf.tile(guide[None, :, :], [batch_size, 1, 1])\n",
        "        guide = (\n",
        "            guide * mask[:, :, None] * mask[:, None, :] +\n",
        "            tf.eye(num_codes)[None, :, :])\n",
        "    prior_guide = None\n",
        "    if use_prior:\n",
        "        prior_values = features['prior_values']\n",
        "        prior_idx_values = prior_values.values\n",
        "\n",
        "        prior_indices = features['prior_indices']\n",
        "        prior_batch_idx = prior_indices.indices[:, 0][::2]\n",
        "        prior_idx = tf.reshape(prior_indices.values, [-1, 2])\n",
        "        prior_idx = tf.concat(\n",
        "            [prior_batch_idx[:, None], prior_idx[:, :1], prior_idx[:, 1:]], axis=1)\n",
        "\n",
        "        temp_idx = (\n",
        "            prior_idx[:, 0] * 1000000 + prior_idx[:, 1] * 1000 + prior_idx[:, 2])\n",
        "        sorted_idx = tf.argsort(temp_idx)\n",
        "        prior_idx = tf.gather(prior_idx, sorted_idx)\n",
        "\n",
        "        prior_idx_shape = [batch_size, max_num_codes * 2, max_num_codes * 2]\n",
        "        sparse_prior = tf.SparseTensor(\n",
        "            indices=prior_idx, values=prior_idx_values, dense_shape=prior_idx_shape)\n",
        "        prior_guide = tf.sparse.to_dense(sparse_prior, validate_indices=True)\n",
        "\n",
        "        visit_guide = tf.convert_to_tensor(\n",
        "            [prior_scalar] * max_num_codes + [0.0] * max_num_codes * 1,\n",
        "            dtype=tf.float32)\n",
        "        prior_guide = tf.concat(\n",
        "            [tf.tile(visit_guide[None, None, :], [batch_size, 1, 1]), prior_guide],\n",
        "            axis=1)\n",
        "        visit_guide = tf.concat([[0.0], visit_guide], axis=0)\n",
        "        prior_guide = tf.concat(\n",
        "            [tf.tile(visit_guide[None, :, None], [batch_size, 1, 1]), prior_guide],\n",
        "            axis=2)\n",
        "        prior_guide = (\n",
        "            prior_guide * mask[:, :, None] * mask[:, None, :] +\n",
        "            prior_scalar * tf.eye(num_codes)[None, :, :])\n",
        "        degrees = tf.reduce_sum(prior_guide, axis=2)\n",
        "        prior_guide = prior_guide / degrees[:, :, None]\n",
        "    return guide,prior_guide\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMmkfpaPMadJ"
      },
      "outputs": [],
      "source": [
        "def get_loss(logits, labels, attentions):\n",
        "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "\n",
        "    if gct_params[\"use_prior\"]:\n",
        "        kl_terms = []\n",
        "        attention_tensor = tf.convert_to_tensor(attentions)\n",
        "        for i in range(1, gct_params[\"num_transformer_stack\"]):\n",
        "            log_p = tf.math.log(attention_tensor[i - 1] + 1e-12)\n",
        "            log_q = tf.math.log(attention_tensor[i] + 1e-12)\n",
        "            kl_term = attention_tensor[i - 1] * (log_p - log_q)\n",
        "            kl_term = tf.reduce_sum(kl_term, axis=-1)\n",
        "            kl_term = tf.reduce_mean(kl_term)\n",
        "            kl_terms.append(kl_term)\n",
        "        reg_term = tf.reduce_mean(kl_terms)\n",
        "        loss += reg_coef * reg_term\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loXKIb5HNgrG"
      },
      "outputs": [],
      "source": [
        "def train_model(dataset, epochs=1):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    for epoch in range(epochs):\n",
        "        # steps_per_epoch=100\n",
        "        for step, (context,features, labels) in enumerate(dataset):\n",
        "            if step>total_samples:\n",
        "                break\n",
        "            with tf.GradientTape() as tape:\n",
        "                embeddings,masks,guide,prior_guide=model.get_embeddings(features,max_num_codes)\n",
        "                logits, attentions = model(embeddings,masks,guide,prior_guide, training=True)\n",
        "                loss= get_loss(logits, labels, attentions)\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            # print(gradients)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4FyfTMeRZwp"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataset):\n",
        "    total_loss = 0\n",
        "    # Initialize metrics here, e.g., accuracy\n",
        "    num_batches = 0\n",
        "    accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "    auc_pr = tf.keras.metrics.AUC(curve='PR', summation_method='interpolation')\n",
        "\n",
        "    # feature_embedder = FeatureEmbedder(self._vocab_sizes, self._feature_keys, self._embedding_size)\n",
        "    acc=0\n",
        "    for context,features, labels in dataset:\n",
        "        embeddings,masks,guide,prior_guide=model.get_embeddings(features,max_num_codes)\n",
        "        logits, attentions = model(embeddings,masks,guide,prior_guide, training=False)\n",
        "        loss = get_loss(logits, labels, attentions)\n",
        "        probs=tf.nn.sigmoid(logits)\n",
        "        predicted_classes = tf.cast(tf.greater_equal(probs, 0.4), tf.int32)\n",
        "        labels_casted = tf.cast(labels, tf.int32)\n",
        "        accuracy_metric.update_state(labels_casted, predicted_classes)\n",
        "        # acc+=accuracy_metric.result().numpy\n",
        "        batch_accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "        batch_accuracy_metric.update_state(labels_casted, predicted_classes)\n",
        "        batch_auc=tf.keras.metrics.AUC(curve='PR', summation_method='interpolation')\n",
        "        batch_auc.update_state(labels_casted, probs)\n",
        "        auc_pr.update_state(labels_casted, probs)\n",
        "        print(\"Batch accuracy:\", batch_accuracy_metric.result().numpy())\n",
        "        print(\"Batch AUC:\", batch_auc.result().numpy())\n",
        "        total_loss += loss.numpy()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    # avg_accuracy = acc/num_batches\n",
        "    avg_accuracy = accuracy_metric.result().numpy()\n",
        "    avg_auc=auc_pr.result().numpy()\n",
        "    print(f\"Eval Loss: {avg_loss}, Eval Accuracy: {avg_accuracy}, Eval AUC: {avg_auc}\")\n",
        "    # return avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzts6YakSo9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5164bb0-0243-4e73-d3b8-7a07bbc03405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7aa508759ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7aa508759ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function GraphConvolutionalTransformer.get_embeddings at 0x7aa508687880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function GraphConvolutionalTransformer.get_embeddings at 0x7aa508687880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.4389280080795288\n",
            "Epoch 1, Loss: 0.4266732633113861\n"
          ]
        }
      ],
      "source": [
        "train_model(train_dataset, epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxIrolR6So6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483eebac-2f78-45f1-c78e-efdf4f094b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04720752\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02360376\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.024305224\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.12732707\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.024305224\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.1465285\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.16666667\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.12732707\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.2608862\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.16037233\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.0919118\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.25\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.174293\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07531468\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.1562506\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.20965737\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.20038633\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07081128\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04861045\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.118410036\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.293057\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10041956\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.28745437\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.16037233\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.20024614\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.3399422\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.12994972\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.12994972\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02360376\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.34937066\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.2730126\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.0972209\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.3633178\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.14490816\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04720752\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.22570701\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.22859767\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.112853505\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.25465414\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.065690346\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.0919118\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1944863\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.35144234\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.07226309\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.097143665\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04482516\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.09441504\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.20965737\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10390846\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.11878501\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.10300119\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.12853502\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.118410036\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.118410036\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.18380323\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.22174211\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04720752\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.4\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.039669745\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.16666667\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.087905586\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.118410036\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.18436164\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04720752\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.16037233\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04861045\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.20578505\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.097143665\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17021185\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.18816057\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1211597\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.16037233\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.0972209\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.094080284\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02360376\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.2865168\n",
            "Batch accuracy: 0.25\n",
            "Batch AUC: 0.81772846\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.53619576\n",
            "Batch accuracy: 0.25\n",
            "Batch AUC: 0.75\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.6471319\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.6733709\n",
            "Batch accuracy: 0.46875\n",
            "Batch AUC: 0.60527766\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.66464335\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.6471319\n",
            "Batch accuracy: 0.15625\n",
            "Batch AUC: 0.84375\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.5772258\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.75298166\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.64125776\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.5625\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.5625\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.69983953\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.6618592\n",
            "Batch accuracy: 0.5\n",
            "Batch AUC: 0.43475243\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.6854446\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.6556215\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.62939066\n",
            "Batch accuracy: 0.46875\n",
            "Batch AUC: 0.4426596\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.7281697\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.5642106\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.6819344\n",
            "Batch accuracy: 0.5\n",
            "Batch AUC: 0.5\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.5136761\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.5772258\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.72957367\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.65881366\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.5625\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.61230505\n",
            "Batch accuracy: 0.1875\n",
            "Batch AUC: 0.7944163\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.7265067\n",
            "Batch accuracy: 0.5\n",
            "Batch AUC: 0.61959165\n",
            "Batch accuracy: 0.25\n",
            "Batch AUC: 0.75\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.62939066\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.6839465\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.7065422\n",
            "Batch accuracy: 0.5\n",
            "Batch AUC: 0.45842686\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.6369358\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.6016852\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.6618592\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.66464335\n",
            "Batch accuracy: 0.5625\n",
            "Batch AUC: 0.4139296\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.6556215\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.71875\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.59375\n",
            "Batch accuracy: 0.5625\n",
            "Batch AUC: 0.3645432\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.5157302\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.5136761\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.5434405\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.71875\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.6875\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.5730336\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.75449127\n",
            "Batch accuracy: 0.5\n",
            "Batch AUC: 0.5\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.6016852\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.5136761\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.79621816\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.5596305\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.625\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.6471319\n",
            "Batch accuracy: 0.5625\n",
            "Batch AUC: 0.45696303\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.77671677\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.65881366\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.60325366\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.75298166\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.70629084\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.6471319\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.7281697\n",
            "Batch accuracy: 0.1875\n",
            "Batch AUC: 0.8495088\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.62939066\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.62939066\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.6471319\n",
            "Batch accuracy: 0.25\n",
            "Batch AUC: 0.7803371\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.5162685\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.75298166\n",
            "Batch accuracy: 0.21875\n",
            "Batch AUC: 0.8250121\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.5443819\n",
            "Batch accuracy: 0.46875\n",
            "Batch AUC: 0.5255737\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.520776\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.5157302\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.60325366\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.5625\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.5443819\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.63033694\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.5443819\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.77671677\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.63033694\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.7265067\n",
            "Batch accuracy: 0.46875\n",
            "Batch AUC: 0.488545\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.68244773\n",
            "Batch accuracy: 0.5\n",
            "Batch AUC: 0.43475243\n",
            "Batch accuracy: 0.21875\n",
            "Batch AUC: 0.839601\n",
            "Batch accuracy: 0.46875\n",
            "Batch AUC: 0.4426596\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.66464335\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.75298166\n",
            "Batch accuracy: 0.25\n",
            "Batch AUC: 0.75\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.6618592\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.5706125\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.66464335\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.77671677\n",
            "Batch accuracy: 0.5\n",
            "Batch AUC: 0.5\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.55132496\n",
            "Batch accuracy: 0.5\n",
            "Batch AUC: 0.61959165\n",
            "Batch accuracy: 0.34375\n",
            "Batch AUC: 0.6016852\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.7533464\n",
            "Batch accuracy: 0.28125\n",
            "Batch AUC: 0.7533464\n",
            "Batch accuracy: 0.3125\n",
            "Batch AUC: 0.60328007\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.6733709\n",
            "Batch accuracy: 0.375\n",
            "Batch AUC: 0.60325366\n",
            "Batch accuracy: 0.40625\n",
            "Batch AUC: 0.53890276\n",
            "Batch accuracy: 0.4375\n",
            "Batch AUC: 0.5625\n",
            "Batch accuracy: 0.5\n",
            "Batch AUC: 0.43828666\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.24454825\n",
            "Batch accuracy: 0.625\n",
            "Batch AUC: 0.40036923\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.19020417\n",
            "Batch accuracy: 0.65625\n",
            "Batch AUC: 0.34375\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.25\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.15327749\n",
            "Batch accuracy: 0.5\n",
            "Batch AUC: 0.5\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.22921343\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.23207135\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.21875\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.34937066\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.24454825\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.5217724\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.16303216\n",
            "Batch accuracy: 0.65625\n",
            "Batch AUC: 0.31516847\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.2578651\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.2865168\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.16303216\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.2578651\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.30979583\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.0919118\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07291568\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.22662812\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.47354627\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.10300119\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1820051\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04720752\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.15625\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.20056175\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.21529078\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.30979583\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04861045\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.1041552\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.14490816\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1875\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.3125\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.59375\n",
            "Batch AUC: 0.37247184\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.1041552\n",
            "Batch accuracy: 0.625\n",
            "Batch AUC: 0.47471917\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.25\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.40627873\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.15625\n",
            "Batch accuracy: 0.59375\n",
            "Batch AUC: 0.37247184\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.29475737\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.27172026\n",
            "Batch accuracy: 0.65625\n",
            "Batch AUC: 0.37649083\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.40049228\n",
            "Batch accuracy: 0.5625\n",
            "Batch AUC: 0.45696303\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.22921343\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.25149712\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.43269673\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.3125\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.43269673\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.3532711\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.22921343\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.2608862\n",
            "Batch accuracy: 0.625\n",
            "Batch AUC: 0.34382012\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.3983147\n",
            "Batch accuracy: 0.65625\n",
            "Batch AUC: 0.31516847\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.5290183\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.2865168\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.15625\n",
            "Batch accuracy: 0.65625\n",
            "Batch AUC: 0.31516847\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.27172026\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.25\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.24454825\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.20056175\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.53125\n",
            "Batch AUC: 0.5996655\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.27172026\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.15062936\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.43058157\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.20313936\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.2861258\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.16037233\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.3794965\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.5290183\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.3983147\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.15623279\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.30979583\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.21737622\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07291568\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.34937066\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07531468\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.16303216\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.1041552\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.16303216\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07531468\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.15062936\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.1822716\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07291568\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.14490816\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.21529078\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.15625\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.38091674\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.16303216\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.5\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04720752\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.43055785\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10041956\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.25201625\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.25617206\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.24411333\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.16631705\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.51293707\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17977986\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.22048438\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.2660189\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.3326341\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.11005828\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.15623279\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.078116395\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.16037233\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.12552446\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.15571567\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.26450914\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.16037233\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07291568\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07081128\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1507353\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.1041552\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07291568\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.19221552\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.25465414\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.078116395\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.078116395\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.15623279\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.34937066\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.26450914\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02360376\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02241258\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04861045\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07291568\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04482516\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.20024614\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.44606754\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.11878501\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.1465285\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.20056175\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.5702248\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.22049855\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.1465285\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.078116395\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.22049855\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10041956\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.11801879\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.14490816\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10041956\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.3326341\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.44606754\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07531468\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.33333334\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.1822716\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07291568\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.078116395\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.4395855\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02360376\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.024305224\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04720752\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07531468\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.130194\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.26450914\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.20313936\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.20024614\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.22049855\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.7373371\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.1822716\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.26450914\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.1465285\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.20056175\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.19189826\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.30979583\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.15623279\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.40049228\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1875\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.30979583\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.5\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.20024614\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.37921363\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.16631705\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.61959165\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.21529078\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.22921343\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.27257615\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.38603544\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.29038724\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.078116395\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.15625\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1875\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.1822716\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.5598132\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.21529078\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.22921343\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.37921363\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.16303216\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.15625\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.15625\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.22049855\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.20024614\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.078116395\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.078116395\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.14490816\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.2608862\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.1041552\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.130194\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.44606754\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.16631705\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02241258\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.024305224\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02360376\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.024305224\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.1465285\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.5\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.024305224\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02297795\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.12994972\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.32060295\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04861045\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.12994972\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.17277813\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.25\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02297795\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07291568\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02297795\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.5\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.12732707\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.16303216\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.14285715\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02297795\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02360376\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.5075407\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.22049855\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04861045\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02241258\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.20517485\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02360376\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.16631705\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.1041552\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.11878501\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.20815852\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07531468\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.14490816\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.1465285\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02297795\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02360376\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.25\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.024305224\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04861045\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.16631705\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.33333334\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07531468\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.32060295\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04720752\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.21529078\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.20815852\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.40007895\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.625\n",
            "Batch AUC: 0.5471329\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.15625\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.43058157\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.25149712\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1875\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.16303216\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.5109036\n",
            "Batch accuracy: 0.53125\n",
            "Batch AUC: 0.68702567\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.3983147\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.3983147\n",
            "Batch accuracy: 0.65625\n",
            "Batch AUC: 0.44652045\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.25149712\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.260388\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1875\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.50493723\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.43269673\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.25149712\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.130194\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.25\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1875\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.28125\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.3309296\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1875\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.21875\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.25\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.28625154\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1875\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.40049228\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.5139227\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.2608862\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.21737622\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.25\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17191006\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.38603544\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.40627873\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.15625\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.4395855\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.16303216\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.21529078\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.2865168\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.3983147\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1875\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.20313936\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.078116395\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.25308055\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.1465285\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.22848152\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.04861045\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07531468\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07531468\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.30979583\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.21529078\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1432584\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.1562506\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.1764706\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.28412443\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.2\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.42105263\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.14090174\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.32359624\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.44976902\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.37525696\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.17581117\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.14452618\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10722202\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.24398164\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1459096\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.1875\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.14577611\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.2818035\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.22048438\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.37860924\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.31224477\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.48631907\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.25054395\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2968387\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.20600238\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.5216721\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1944863\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.32311484\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0459559\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.23399103\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.2660189\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.28625154\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.34674978\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.25308055\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.097143665\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.1507353\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.22859767\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.34041113\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.12732707\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.22174211\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.097143665\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.5103729\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.25989944\n",
            "Batch accuracy: 0.6875\n",
            "Batch AUC: 0.27874896\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.34847683\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.42986342\n",
            "Batch accuracy: 0.78125\n",
            "Batch AUC: 0.22048438\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.12732707\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.130194\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0625\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10041956\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.38112372\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.26450914\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.15625\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.1562506\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.09375\n",
            "Batch accuracy: 0.71875\n",
            "Batch AUC: 0.3309296\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.09441504\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08595503\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.38112372\n",
            "Batch accuracy: 0.75\n",
            "Batch AUC: 0.20083912\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.44606754\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.44606754\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.02510489\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.07291568\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.0520776\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10041956\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.05020978\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.10868811\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.12994972\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.03125\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 1.0\n",
            "Batch AUC: 0.0\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.057303358\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.0972209\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.028651679\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.26450914\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.0260388\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.21529078\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.84375\n",
            "Batch AUC: 0.13586013\n",
            "Batch accuracy: 0.8125\n",
            "Batch AUC: 0.16303216\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.114606716\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.1041552\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.125\n",
            "Batch accuracy: 0.9375\n",
            "Batch AUC: 0.054344054\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.14490816\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.08151608\n",
            "Batch accuracy: 0.96875\n",
            "Batch AUC: 0.027172027\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.14490816\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.44606754\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.17468533\n",
            "Batch accuracy: 0.875\n",
            "Batch AUC: 0.2554518\n",
            "Batch accuracy: 0.90625\n",
            "Batch AUC: 0.078116395\n",
            "Batch accuracy: 0.9\n",
            "Batch AUC: 0.074705064\n",
            "Eval Loss: 0.5805171081545757, Eval Accuracy: 0.8291590213775635, Eval AUC: 0.17561008036136627\n"
          ]
        }
      ],
      "source": [
        "avg_accuracy = evaluate_model(model, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPcuiRHRSo3K"
      },
      "outputs": [],
      "source": [
        "num_of_advsamples=30 #10 batches of test data samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mq1wvSx4J9A"
      },
      "outputs": [],
      "source": [
        "adv_samples=test_dataset.take(num_of_advsamples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dgCzqsSyzhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9005aa5a-bc88-4ff0-c878-51e4eef72cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dx_ints SparseTensor(indices=tf.Tensor(\n",
            "[[ 0  0  0]\n",
            " [ 0  0  1]\n",
            " [ 0  0  2]\n",
            " [ 0  0  3]\n",
            " [ 0  0  4]\n",
            " [ 0  0  5]\n",
            " [ 1  0  0]\n",
            " [ 1  0  1]\n",
            " [ 1  0  2]\n",
            " [ 1  0  3]\n",
            " [ 1  0  4]\n",
            " [ 1  0  5]\n",
            " [ 1  0  6]\n",
            " [ 1  0  7]\n",
            " [ 2  0  0]\n",
            " [ 2  0  1]\n",
            " [ 2  0  2]\n",
            " [ 2  0  3]\n",
            " [ 2  0  4]\n",
            " [ 2  0  5]\n",
            " [ 3  0  0]\n",
            " [ 3  0  1]\n",
            " [ 3  0  2]\n",
            " [ 3  0  3]\n",
            " [ 4  0  0]\n",
            " [ 4  0  1]\n",
            " [ 4  0  2]\n",
            " [ 4  0  3]\n",
            " [ 4  0  4]\n",
            " [ 4  0  5]\n",
            " [ 5  0  0]\n",
            " [ 5  0  1]\n",
            " [ 5  0  2]\n",
            " [ 5  0  3]\n",
            " [ 5  0  4]\n",
            " [ 6  0  0]\n",
            " [ 6  0  1]\n",
            " [ 6  0  2]\n",
            " [ 6  0  3]\n",
            " [ 7  0  0]\n",
            " [ 7  0  1]\n",
            " [ 7  0  2]\n",
            " [ 7  0  3]\n",
            " [ 7  0  4]\n",
            " [ 7  0  5]\n",
            " [ 8  0  0]\n",
            " [ 8  0  1]\n",
            " [ 8  0  2]\n",
            " [ 8  0  3]\n",
            " [ 8  0  4]\n",
            " [ 8  0  5]\n",
            " [ 8  0  6]\n",
            " [ 8  0  7]\n",
            " [ 8  0  8]\n",
            " [ 8  0  9]\n",
            " [ 9  0  0]\n",
            " [ 9  0  1]\n",
            " [ 9  0  2]\n",
            " [ 9  0  3]\n",
            " [ 9  0  4]\n",
            " [ 9  0  5]\n",
            " [ 9  0  6]\n",
            " [ 9  0  7]\n",
            " [ 9  0  8]\n",
            " [10  0  0]\n",
            " [10  0  1]\n",
            " [10  0  2]\n",
            " [10  0  3]\n",
            " [10  0  4]\n",
            " [10  0  5]\n",
            " [11  0  0]\n",
            " [11  0  1]\n",
            " [11  0  2]\n",
            " [11  0  3]\n",
            " [11  0  4]\n",
            " [11  0  5]\n",
            " [11  0  6]\n",
            " [12  0  0]\n",
            " [12  0  1]\n",
            " [12  0  2]\n",
            " [12  0  3]\n",
            " [12  0  4]\n",
            " [13  0  0]\n",
            " [13  0  1]\n",
            " [13  0  2]\n",
            " [13  0  3]\n",
            " [13  0  4]\n",
            " [13  0  5]\n",
            " [13  0  6]\n",
            " [13  0  7]\n",
            " [14  0  0]\n",
            " [14  0  1]\n",
            " [14  0  2]\n",
            " [14  0  3]\n",
            " [14  0  4]\n",
            " [14  0  5]\n",
            " [15  0  0]\n",
            " [15  0  1]\n",
            " [15  0  2]\n",
            " [15  0  3]\n",
            " [15  0  4]\n",
            " [15  0  5]\n",
            " [16  0  0]\n",
            " [16  0  1]\n",
            " [16  0  2]\n",
            " [16  0  3]\n",
            " [16  0  4]\n",
            " [16  0  5]\n",
            " [16  0  6]\n",
            " [16  0  7]\n",
            " [16  0  8]\n",
            " [17  0  0]\n",
            " [17  0  1]\n",
            " [17  0  2]\n",
            " [17  0  3]\n",
            " [17  0  4]\n",
            " [18  0  0]\n",
            " [18  0  1]\n",
            " [18  0  2]\n",
            " [18  0  3]\n",
            " [18  0  4]\n",
            " [18  0  5]\n",
            " [18  0  6]\n",
            " [18  0  7]\n",
            " [18  0  8]\n",
            " [18  0  9]\n",
            " [19  0  0]\n",
            " [19  0  1]\n",
            " [19  0  2]\n",
            " [19  0  3]\n",
            " [19  0  4]\n",
            " [19  0  5]\n",
            " [19  0  6]\n",
            " [19  0  7]\n",
            " [20  0  0]\n",
            " [20  0  1]\n",
            " [20  0  2]\n",
            " [20  0  3]\n",
            " [20  0  4]\n",
            " [21  0  0]\n",
            " [21  0  1]\n",
            " [21  0  2]\n",
            " [21  0  3]\n",
            " [21  0  4]\n",
            " [21  0  5]\n",
            " [21  0  6]\n",
            " [21  0  7]\n",
            " [21  0  8]\n",
            " [22  0  0]\n",
            " [22  0  1]\n",
            " [22  0  2]\n",
            " [22  0  3]\n",
            " [22  0  4]\n",
            " [22  0  5]\n",
            " [22  0  6]\n",
            " [22  0  7]\n",
            " [22  0  8]\n",
            " [23  0  0]\n",
            " [23  0  1]\n",
            " [23  0  2]\n",
            " [23  0  3]\n",
            " [23  0  4]\n",
            " [23  0  5]\n",
            " [23  0  6]\n",
            " [23  0  7]\n",
            " [23  0  8]\n",
            " [23  0  9]\n",
            " [23  0 10]\n",
            " [23  0 11]\n",
            " [23  0 12]\n",
            " [23  0 13]\n",
            " [23  0 14]\n",
            " [24  0  0]\n",
            " [24  0  1]\n",
            " [24  0  2]\n",
            " [24  0  3]\n",
            " [25  0  0]\n",
            " [25  0  1]\n",
            " [25  0  2]\n",
            " [25  0  3]\n",
            " [25  0  4]\n",
            " [26  0  0]\n",
            " [26  0  1]\n",
            " [26  0  2]\n",
            " [26  0  3]\n",
            " [26  0  4]\n",
            " [26  0  5]\n",
            " [26  0  6]\n",
            " [26  0  7]\n",
            " [26  0  8]\n",
            " [26  0  9]\n",
            " [26  0 10]\n",
            " [27  0  0]\n",
            " [27  0  1]\n",
            " [27  0  2]\n",
            " [27  0  3]\n",
            " [27  0  4]\n",
            " [27  0  5]\n",
            " [27  0  6]\n",
            " [28  0  0]\n",
            " [28  0  1]\n",
            " [28  0  2]\n",
            " [28  0  3]\n",
            " [28  0  4]\n",
            " [28  0  5]\n",
            " [28  0  6]\n",
            " [28  0  7]\n",
            " [28  0  8]\n",
            " [29  0  0]\n",
            " [29  0  1]\n",
            " [29  0  2]\n",
            " [29  0  3]\n",
            " [29  0  4]\n",
            " [29  0  5]\n",
            " [29  0  6]\n",
            " [29  0  7]\n",
            " [29  0  8]\n",
            " [29  0  9]\n",
            " [29  0 10]\n",
            " [29  0 11]\n",
            " [29  0 12]\n",
            " [29  0 13]\n",
            " [30  0  0]\n",
            " [30  0  1]\n",
            " [30  0  2]\n",
            " [30  0  3]\n",
            " [30  0  4]\n",
            " [30  0  5]\n",
            " [30  0  6]\n",
            " [30  0  7]\n",
            " [30  0  8]\n",
            " [31  0  0]\n",
            " [31  0  1]\n",
            " [31  0  2]\n",
            " [31  0  3]], shape=(235, 3), dtype=int64), values=tf.Tensor(\n",
            "[  3   1   0   2   4   5  12  15   6  14  13  17   2  16  20  18   6  19\n",
            "   2  21   2   6  27  28  29  32  31   2  30  33  43  40   2  41  42  44\n",
            "  41   2  45   8   6  47  46   2   9  48  50  52  53  54   6  36   2  49\n",
            "  51  58  57  56  55   8   6   7   2   9  61  41  62  63  14   2   3  65\n",
            "  67  64   0  66   2  20   6   2  68  69  12  70  35  71   3   6  72   2\n",
            "  75  74  76   6   2  73  77  56  23  13   2  78  12  79  15  54  67   6\n",
            "  14  36   2  81  82  80   2  41  83  90  86  91  87  85  20  84  88  89\n",
            "  96  48  99  95   6  97  98   2 100   8 101   6   2 105 102 108 106 103\n",
            " 107 109   2 104  12 111  35  50  56   6 110 112   2 117 120  48 114   3\n",
            " 119  63 115 116   6  72 118 113  17   2 121   2 122 103 124  41 123   2\n",
            " 125 128 126 130 131 127  54 129   6 132  88   2  86 133 135 134 136  84\n",
            " 119  12  71 140 138   6  72 137   2 139 145 144 147 130 119  54  63 149\n",
            "   6  72 146  14   2 148 152 150 106 101   6 151   2  21  69  41   2 153\n",
            " 154], shape=(235,), dtype=int64), dense_shape=tf.Tensor([32  1 15], shape=(3,), dtype=int64))\n",
            "prior_indices SparseTensor(indices=tf.Tensor(\n",
            "[[  0   0   0]\n",
            " [  0   0   1]\n",
            " [  0   0   2]\n",
            " ...\n",
            " [ 31   0 173]\n",
            " [ 31   0 174]\n",
            " [ 31   0 175]], shape=(8020, 3), dtype=int64), values=tf.Tensor([ 0 50  0 ...  2 60  3], shape=(8020,), dtype=int64), dense_shape=tf.Tensor([ 32   1 936], shape=(3,), dtype=int64))\n",
            "prior_values SparseTensor(indices=tf.Tensor(\n",
            "[[ 0  0  0]\n",
            " [ 0  0  1]\n",
            " [ 0  0  2]\n",
            " ...\n",
            " [31  0 85]\n",
            " [31  0 86]\n",
            " [31  0 87]], shape=(4010, 3), dtype=int64), values=tf.Tensor([0.07182018 0.02741228 0.03892544 ... 0.7355372  0.00826446 0.00516529], shape=(4010,), dtype=float32), dense_shape=tf.Tensor([ 32   1 468], shape=(3,), dtype=int64))\n",
            "proc_ints SparseTensor(indices=tf.Tensor(\n",
            "[[ 0  0  0]\n",
            " [ 0  0  1]\n",
            " [ 0  0  2]\n",
            " [ 0  0  3]\n",
            " [ 0  0  4]\n",
            " [ 0  0  5]\n",
            " [ 1  0  0]\n",
            " [ 1  0  1]\n",
            " [ 2  0  0]\n",
            " [ 2  0  1]\n",
            " [ 2  0  2]\n",
            " [ 2  0  3]\n",
            " [ 2  0  4]\n",
            " [ 2  0  5]\n",
            " [ 2  0  6]\n",
            " [ 2  0  7]\n",
            " [ 3  0  0]\n",
            " [ 3  0  1]\n",
            " [ 3  0  2]\n",
            " [ 3  0  3]\n",
            " [ 3  0  4]\n",
            " [ 3  0  5]\n",
            " [ 4  0  0]\n",
            " [ 4  0  1]\n",
            " [ 4  0  2]\n",
            " [ 5  0  0]\n",
            " [ 5  0  1]\n",
            " [ 5  0  2]\n",
            " [ 5  0  3]\n",
            " [ 5  0  4]\n",
            " [ 5  0  5]\n",
            " [ 5  0  6]\n",
            " [ 5  0  7]\n",
            " [ 6  0  0]\n",
            " [ 6  0  1]\n",
            " [ 7  0  0]\n",
            " [ 7  0  1]\n",
            " [ 7  0  2]\n",
            " [ 7  0  3]\n",
            " [ 7  0  4]\n",
            " [ 7  0  5]\n",
            " [ 7  0  6]\n",
            " [ 7  0  7]\n",
            " [ 7  0  8]\n",
            " [ 8  0  0]\n",
            " [ 8  0  1]\n",
            " [ 8  0  2]\n",
            " [ 8  0  3]\n",
            " [ 8  0  4]\n",
            " [ 9  0  0]\n",
            " [ 9  0  1]\n",
            " [ 9  0  2]\n",
            " [ 9  0  3]\n",
            " [ 9  0  4]\n",
            " [ 9  0  5]\n",
            " [ 9  0  6]\n",
            " [10  0  0]\n",
            " [10  0  1]\n",
            " [10  0  2]\n",
            " [10  0  3]\n",
            " [11  0  0]\n",
            " [11  0  1]\n",
            " [11  0  2]\n",
            " [11  0  3]\n",
            " [11  0  4]\n",
            " [12  0  0]\n",
            " [12  0  1]\n",
            " [12  0  2]\n",
            " [12  0  3]\n",
            " [12  0  4]\n",
            " [13  0  0]\n",
            " [13  0  1]\n",
            " [13  0  2]\n",
            " [13  0  3]\n",
            " [14  0  0]\n",
            " [14  0  1]\n",
            " [14  0  2]\n",
            " [14  0  3]\n",
            " [14  0  4]\n",
            " [14  0  5]\n",
            " [15  0  0]\n",
            " [15  0  1]\n",
            " [15  0  2]\n",
            " [15  0  3]\n",
            " [15  0  4]\n",
            " [15  0  5]\n",
            " [15  0  6]\n",
            " [16  0  0]\n",
            " [16  0  1]\n",
            " [16  0  2]\n",
            " [16  0  3]\n",
            " [16  0  4]\n",
            " [16  0  5]\n",
            " [16  0  6]\n",
            " [16  0  7]\n",
            " [16  0  8]\n",
            " [16  0  9]\n",
            " [16  0 10]\n",
            " [16  0 11]\n",
            " [16  0 12]\n",
            " [16  0 13]\n",
            " [16  0 14]\n",
            " [17  0  0]\n",
            " [17  0  1]\n",
            " [17  0  2]\n",
            " [17  0  3]\n",
            " [17  0  4]\n",
            " [17  0  5]\n",
            " [17  0  6]\n",
            " [17  0  7]\n",
            " [17  0  8]\n",
            " [17  0  9]\n",
            " [17  0 10]\n",
            " [17  0 11]\n",
            " [17  0 12]\n",
            " [18  0  0]\n",
            " [18  0  1]\n",
            " [18  0  2]\n",
            " [18  0  3]\n",
            " [18  0  4]\n",
            " [18  0  5]\n",
            " [18  0  6]\n",
            " [18  0  7]\n",
            " [18  0  8]\n",
            " [18  0  9]\n",
            " [18  0 10]\n",
            " [18  0 11]\n",
            " [19  0  0]\n",
            " [19  0  1]\n",
            " [19  0  2]\n",
            " [19  0  3]\n",
            " [19  0  4]\n",
            " [19  0  5]\n",
            " [19  0  6]\n",
            " [19  0  7]\n",
            " [20  0  0]\n",
            " [20  0  1]\n",
            " [20  0  2]\n",
            " [20  0  3]\n",
            " [20  0  4]\n",
            " [20  0  5]\n",
            " [21  0  0]\n",
            " [21  0  1]\n",
            " [21  0  2]\n",
            " [21  0  3]\n",
            " [21  0  4]\n",
            " [21  0  5]\n",
            " [21  0  6]\n",
            " [21  0  7]\n",
            " [21  0  8]\n",
            " [21  0  9]\n",
            " [21  0 10]\n",
            " [21  0 11]\n",
            " [21  0 12]\n",
            " [21  0 13]\n",
            " [21  0 14]\n",
            " [22  0  0]\n",
            " [22  0  1]\n",
            " [22  0  2]\n",
            " [22  0  3]\n",
            " [23  0  0]\n",
            " [23  0  1]\n",
            " [23  0  2]\n",
            " [23  0  3]\n",
            " [23  0  4]\n",
            " [23  0  5]\n",
            " [23  0  6]\n",
            " [23  0  7]\n",
            " [23  0  8]\n",
            " [24  0  0]\n",
            " [24  0  1]\n",
            " [25  0  0]\n",
            " [25  0  1]\n",
            " [25  0  2]\n",
            " [25  0  3]\n",
            " [25  0  4]\n",
            " [25  0  5]\n",
            " [25  0  6]\n",
            " [26  0  0]\n",
            " [26  0  1]\n",
            " [26  0  2]\n",
            " [26  0  3]\n",
            " [26  0  4]\n",
            " [26  0  5]\n",
            " [26  0  6]\n",
            " [26  0  7]\n",
            " [26  0  8]\n",
            " [26  0  9]\n",
            " [26  0 10]\n",
            " [26  0 11]\n",
            " [26  0 12]\n",
            " [26  0 13]\n",
            " [26  0 14]\n",
            " [26  0 15]\n",
            " [26  0 16]\n",
            " [26  0 17]\n",
            " [27  0  0]\n",
            " [27  0  1]\n",
            " [27  0  2]\n",
            " [27  0  3]\n",
            " [27  0  4]\n",
            " [27  0  5]\n",
            " [27  0  6]\n",
            " [28  0  0]\n",
            " [28  0  1]\n",
            " [28  0  2]\n",
            " [28  0  3]\n",
            " [28  0  4]\n",
            " [28  0  5]\n",
            " [28  0  6]\n",
            " [28  0  7]\n",
            " [28  0  8]\n",
            " [28  0  9]\n",
            " [28  0 10]\n",
            " [28  0 11]\n",
            " [28  0 12]\n",
            " [28  0 13]\n",
            " [28  0 14]\n",
            " [28  0 15]\n",
            " [28  0 16]\n",
            " [28  0 17]\n",
            " [28  0 18]\n",
            " [28  0 19]\n",
            " [28  0 20]\n",
            " [28  0 21]\n",
            " [28  0 22]\n",
            " [28  0 23]\n",
            " [28  0 24]\n",
            " [28  0 25]\n",
            " [29  0  0]\n",
            " [29  0  1]\n",
            " [29  0  2]\n",
            " [29  0  3]\n",
            " [29  0  4]\n",
            " [29  0  5]\n",
            " [29  0  6]\n",
            " [29  0  7]\n",
            " [30  0  0]\n",
            " [30  0  1]\n",
            " [30  0  2]\n",
            " [30  0  3]\n",
            " [30  0  4]\n",
            " [30  0  5]\n",
            " [31  0  0]\n",
            " [31  0  1]\n",
            " [31  0  2]\n",
            " [31  0  3]\n",
            " [31  0  4]\n",
            " [31  0  5]\n",
            " [31  0  6]\n",
            " [31  0  7]\n",
            " [31  0  8]\n",
            " [31  0  9]\n",
            " [31  0 10]], shape=(254, 3), dtype=int64), values=tf.Tensor(\n",
            "[  2   3   0   4   5   1  15  16  17  12  20  19  18  22  23  21  13  30\n",
            "  31  20  18  19  33  32  34  48  53  20  51  47  50  49  52  55  54  13\n",
            "  58  40  57  10   8   6  21  56  13  61  60  32  59  43  62   9  11  12\n",
            "   8  63  67  68  15  32  72  73  69  71  70  75  37   2  74   4  25  29\n",
            "  26  27  58  78  76  23  77  79  81  35  60  32   0  82  80  39  15  16\n",
            "  87  90  35  86  89  88  60  32  84  85  82  83  13  98  90  93  94  99\n",
            "  11  19  91  95  96  92  97 100  13  80  62  31  18  95   4 101  96  24\n",
            "  97 109  20  19  34  32  85 107 108  58  40 111 110   8  77  17  64  81\n",
            "  90  62 113  22  41  65   8  60   4  21 112  97 114  15  16  24 116 115\n",
            " 117 118  86  66 102  85 107 119  36  64 122 121  20  19 120  63  39 125\n",
            " 128 129  62  86  20  19  66 127 123  32  85 126   4 124  44 112  81  62\n",
            "  55  32 130  80 112 133 132 137  25 136  39 134 147 131 144 139  40 148\n",
            " 142  85 143 140 141  94  86 138 146 135  82  44 145 132 153 146  66 151\n",
            " 102  85 152  72 154  18  41  82  77  62 155 157  20  22 156 158  63   4\n",
            "  52 112], shape=(254,), dtype=int64), dense_shape=tf.Tensor([32  1 26], shape=(3,), dtype=int64))\n"
          ]
        }
      ],
      "source": [
        "for c,f,l in adv_samples:\n",
        "    for key,values in f.items():\n",
        "        print(key,values)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pimQen404NPD"
      },
      "outputs": [],
      "source": [
        "def fgsm_attack(dataset, epochs=1):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.022)\n",
        "    num_batches = 0\n",
        "    total_loss = 0\n",
        "    accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "    auc_pr = tf.keras.metrics.AUC(curve='PR', summation_method='interpolation')\n",
        "    for epoch in range(epochs):\n",
        "        epsilon = 0.002\n",
        "        for step, (context,features, labels) in enumerate(dataset):\n",
        "            with tf.GradientTape() as tape:\n",
        "                tape.watch(features)\n",
        "                embeddings,masks,guide,prior_guide=model.get_embeddings(features,max_num_codes)\n",
        "                logits, attentions = model(embeddings, masks, guide, prior_guide, training=False)\n",
        "                loss= get_loss(logits, labels, attentions)\n",
        "            batch_size, dynamic_second_dim, embedding_size = embeddings.shape\n",
        "            gradients = tape.gradient(loss, logits)\n",
        "            perturbation = epsilon * tf.sign(gradients)\n",
        "\n",
        "            perturbations_expanded = tf.expand_dims(tf.expand_dims(perturbation, -1), -1)\n",
        "\n",
        "            # Tile the expanded 'perturbations' to fill up to the shape of 'embeddings'\n",
        "            perturbations_tiled = tf.tile(perturbations_expanded, [1, dynamic_second_dim, embedding_size])\n",
        "\n",
        "            # Apply the perturbations to the embeddings\n",
        "            adversarial_embeddings = embeddings + perturbations_tiled\n",
        "\n",
        "\n",
        "            advlogits, attentions = model(adversarial_embeddings,masks,guide,prior_guide, training=False)\n",
        "            advloss = get_loss(logits, labels, attentions)\n",
        "            probs=tf.nn.sigmoid(advlogits)\n",
        "            predicted_classes = tf.cast(tf.greater_equal(probs, 0.5), tf.int32)\n",
        "            labels_casted = tf.cast(labels, tf.int32)\n",
        "            accuracy_metric.update_state(labels_casted, predicted_classes)\n",
        "            # acc+=accuracy_metric.result().numpy\n",
        "            batch_accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "            batch_accuracy_metric.update_state(labels_casted, predicted_classes)\n",
        "            batch_auc=tf.keras.metrics.AUC(curve='PR', summation_method='interpolation')\n",
        "            batch_auc.update_state(labels_casted, probs)\n",
        "            auc_pr.update_state(labels_casted, probs)\n",
        "            print(\"Adversarial Batch accuracy:\", batch_accuracy_metric.result().numpy())\n",
        "            print(\"Adversarial Batch AUC:\", batch_auc.result().numpy())\n",
        "            total_loss += advloss.numpy()\n",
        "            num_batches += 1\n",
        "        avg_loss = total_loss / num_batches\n",
        "    # avg_accuracy = acc/num_batches\n",
        "        avg_accuracy = accuracy_metric.result().numpy()\n",
        "        avg_auc=auc_pr.result().numpy()\n",
        "        print(f\"Eval Loss: {avg_loss}, Eval Accuracy: {avg_accuracy}, Eval AUC: {avg_auc}\")\n",
        "\n",
        "            # fgsm_examples = features + epsilon * tf.sign(gradients)\n",
        "            # print(gradients)\n",
        "            # optimizer.apply_gradients(zip(gradients, features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QptSMPQrLX0g",
        "outputId": "01b37739-cac6-44fc-d038-d0a30c404e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial Batch accuracy: 1.0\n",
            "Adversarial Batch AUC: 0.0\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.2\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.027172027\n",
            "Adversarial Batch accuracy: 0.9375\n",
            "Adversarial Batch AUC: 0.04720752\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.02510489\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.02360376\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.2\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.024305224\n",
            "Adversarial Batch accuracy: 1.0\n",
            "Adversarial Batch AUC: 0.0\n",
            "Adversarial Batch accuracy: 0.9375\n",
            "Adversarial Batch AUC: 0.12732707\n",
            "Adversarial Batch accuracy: 1.0\n",
            "Adversarial Batch AUC: 0.0\n",
            "Adversarial Batch accuracy: 0.9375\n",
            "Adversarial Batch AUC: 0.054344054\n",
            "Adversarial Batch accuracy: 1.0\n",
            "Adversarial Batch AUC: 0.0\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.024305224\n",
            "Adversarial Batch accuracy: 1.0\n",
            "Adversarial Batch AUC: 0.0\n",
            "Adversarial Batch accuracy: 1.0\n",
            "Adversarial Batch AUC: 0.0\n",
            "Adversarial Batch accuracy: 0.9375\n",
            "Adversarial Batch AUC: 0.0520776\n",
            "Adversarial Batch accuracy: 0.9375\n",
            "Adversarial Batch AUC: 0.1465285\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.16666667\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.0260388\n",
            "Adversarial Batch accuracy: 1.0\n",
            "Adversarial Batch AUC: 0.0\n",
            "Adversarial Batch accuracy: 0.9375\n",
            "Adversarial Batch AUC: 0.12732707\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.02510489\n",
            "Adversarial Batch accuracy: 0.84375\n",
            "Adversarial Batch AUC: 0.2608862\n",
            "Adversarial Batch accuracy: 0.84375\n",
            "Adversarial Batch AUC: 0.16037233\n",
            "Adversarial Batch accuracy: 0.875\n",
            "Adversarial Batch AUC: 0.0919118\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.125\n",
            "Adversarial Batch accuracy: 0.96875\n",
            "Adversarial Batch AUC: 0.25\n",
            "Adversarial Batch accuracy: 0.90625\n",
            "Adversarial Batch AUC: 0.08595503\n",
            "Adversarial Batch accuracy: 0.875\n",
            "Adversarial Batch AUC: 0.174293\n",
            "Eval Loss: 0.33542574048042295, Eval Accuracy: 0.953125, Eval AUC: 0.06505322456359863\n"
          ]
        }
      ],
      "source": [
        "adversarial_data =fgsm_attack(adv_samples, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trr1P8QYSiBP"
      },
      "outputs": [],
      "source": [
        "def fgsm_attack(self,model, feature_embedder, features, labels, epsilon=0.01):\n",
        "    # Convert features to embeddings\n",
        "        # original_embeddings, _ = feature_embedder.lookup(features, self._max_num_codes)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(features)\n",
        "            logits, attentions = self.get_prediction(model, feature_embedder, features, training=False)\n",
        "            loss = self.get_loss(logits, labels,attentions)\n",
        "\n",
        "        # Calculate gradients of loss with respect to embeddings\n",
        "        gradients = tape.gradient(loss, features)\n",
        "\n",
        "        # FGSM attack: Add sign of gradients to the embeddings\n",
        "        perturbed_features = {k: v + epsilon * tf.sign(gradients[k]) for k, v in features.items()}\n",
        "\n",
        "        return perturbed_features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Data\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Data\n",
        "models = ['Test Data', 'Adversarial Data', 'Adversarial Training Data']\n",
        "accuracy_eICU = [test_data_eicu, adv_data_eicu,adv_train_eicu]\n",
        "accuracy_MIMIC = [test_data_mimic, adv_data_mimic,adv_train_mimic]\n",
        "\n",
        "# Assigning bar positions\n",
        "barWidth = 0.35\n",
        "r1 = np.arange(len(accuracy_eICU))\n",
        "r2 = [x + barWidth for x in r1]\n",
        "\n",
        "# Plotting accuracy for eICU\n",
        "ax.bar(r1, accuracy_eICU, color='yellow', width=barWidth, label='eICU Accuracy')\n",
        "\n",
        "# Plotting accuracy for MIMIC\n",
        "ax.bar(r2, accuracy_MIMIC, color='grey', width=barWidth, label='MIMIC Accuracy')\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Data Condition', fontweight='bold')\n",
        "ax.set_ylabel('Accuracy (%)', fontweight='bold')\n",
        "ax.set_xticks([r + barWidth / 2 - 0.175 for r in range(len(accuracy_eICU))])\n",
        "ax.set_xticklabels(models)\n",
        "ax.set_title('Accuracy Comparison between eICU and MIMIC Datasets')\n",
        "\n",
        "# Creating legend & Show graphic\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "# Recreating the plots for the eICU and MIMIC dataset accuracy\n",
        "# Setting up the figure and axes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "WOzBKyDNc2jw",
        "outputId": "8c6e463b-6c8b-4baa-eaa7-383336f25419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fHxmLjVsc6DB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}